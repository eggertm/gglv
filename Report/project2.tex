\documentclass[a4paper]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{float}
\usepackage{amsmath}
\DeclareMathOperator*{\Max}{Max}
\usepackage{amssymb}
\usepackage{varwidth}
\usepackage{graphicx}
\usepackage{csquotes}
\usepackage{dirtytalk}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{listings}
\lstset{
  basicstyle=\ttfamily,
  columns=fullflexible,
  frame=single,
  breaklines=true,
  postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space},
}
\newcommand\todo[1]{\textcolor{red}{TODO: #1}}

\title{REI602M Machine Learning: Allstate Claims Severity}

\author{Eggert Jon Magnusson and Thor Tomasarson}

\date{\today}

\begin{document}
\maketitle

\begin{abstract}
The goal of this assignment is to find a good regression model that predicts the severity or loss for each insurance claim made by customers of the Allstate insurance.

\begin{displayquote}
Dataset Information: \\\\
\say{Allstate is currently developing automated methods of predicting the cost, and hence severity, of claims. In this recruitment challenge, Kagglers are invited to show off their creativity and flex their technical chops by creating an algorithm which accurately predicts claims severity. Aspiring competitors will demonstrate insight into better ways to predict claims severity for the chance to be part of Allstate‘s efforts to ensure a worry-free customer experience.}
\end{displayquote}

\end{abstract}

\section{Introduction}

There are quite many methods available that serve as regression models. The methods tested out in this report are: \\

\begin{varwidth}[t]{.5\textwidth}
    Linear:
    \begin{itemize}
        \item Linear Regression
        \item Ridge
        \item Lasso
        \item Elastic-Net
    \end{itemize}
\end{varwidth}
\hspace{4em}
\begin{varwidth}[t]{.5\textwidth}
    Non-linear:
    \begin{itemize}
        \item K-Neighbors Regression
        \item Support Vector Regression
        \item Bagging Regression
        \item Random Forest Regression
        \item Extra Trees Regression
        \item MLP (Neural Network)
    \end{itemize}
\end{varwidth} \\\\
These models require different kind of preprocessing of the data in order to be efficient. The need for preprocessing ranges from next to no preprocessing up to extensive investigation of each attributes contribution to the output.

To start with each of these models where tested out with default parameters to get a feel of there potential. Then a extensive evaluation of different hyper-parameters where tested out for the more promising methods with various degree of preprocessing.

All experiments where done with Python in JuPyter, the methods tested where gotten from the Scikit-Learn library except for the neural network that was implemented with TensorFlow.

% 0.5 til 1 bls.
% * Hvað var gert?
% * Hvers vegna?
% * Hvernig?

\section{Implementation}

The work methodology for each regression model was as follows:
\begin{enumerate}
  \item Read the data in to memory
  \item Peprocess the data
  \item Test out various hyper-parameters
  \item Evaluate the results with test data
  \item Go back to step 2, if there seem to be need for it
\end{enumerate}

\subsection{Linear models}
The linear models are sensitive to the input data. They can perform badly if the input data contains correlated attributes and they expect a normally distributed output.

The basic ideal behind the linear models are that they are solving equation~\ref{eq:1}, which is minimizing $L^2$-norm error of a hyperplane.
\begin{equation} \label{eq:1}
\min_{\omega\in\mathbb{R}^p}||X\omega - y||_2^2
\end{equation}
Where $X$ is the input data, $p$ is the input dimension, $\omega$ are the model parameters and $y$ is the output data, in our case it is ‘loss’. \\\\
\textbf{Linear regression} simply solves for equation \ref{eq:1} with no regularization of the model parameters. This can lead to severe overfitting of the training data and is sensitive the input data used. \\\\
\textbf{Ridge regression} adds in the $L^2$-norm penalty, which changes the problem to minimizing the error with equation~\ref{eq:2}.
\begin{equation} \label{eq:2}
\min_{\omega\in\mathbb{R}^p} \{||X\omega - y||_2^2 + \lambda_2*||\omega||_2^2 \}
\end{equation}
This forces the model parameters to be as small as possible.\\\\
\textbf{Lasso regression} adds in the $L^1$-norm penalty, and is thus solving for minimum of equation~\ref{eq:3}
\begin{equation} \label{eq:3}
\min_{\omega\in\mathbb{R}^p}  \{||X\omega - y||_2^2 + \lambda_1*||\omega||_1\}
\end{equation}
This forces some of the model parameters to be zero, thus removing unneeded attributes. \\\\
\textbf{Elastic-Net} combines the regularization of both Ridge and Lasso, see equation~\ref{eq:4}. By that it combines the effect of $L^1$-norm to force unneeded parameters to zero and the $L^2$-norm to hold of a little information from that parameter (no setting to complete zero).
\begin{equation} \label{eq:4}
\min_{\omega\in\mathbb{R}^p}  \{||X\omega - y||_2^2 + \lambda_1*||\omega||_1 + \lambda_2*||\omega||_2^2\}
\end{equation}

All these models are solving for minimum of error with $L^2$-norm, this counters the objective which is the find model with the minimum of the absolute mean error ($L^1$-norm).

\subsection{Non-linear models}
The non-linear models differ quite a lot on what there objective is and how they achieve it. \\\\
\textbf{k-nearest neighbors} learns the data set and estimates value predictions based on averaging the k nearest neighbors output values. \\\\
\textbf{Support vector regression} solves for the “soft margin” with slack variables $\xi_n$ and $\xi_n^*$ that allow for violation on both sides. The problem can be formulated as:
\begin{align} \label{eq:5}
    &\min_{\omega} \{\frac{1}{2}\omega^T\omega + C\sum_i(\xi_n + \xi_n^*)\} \notag \\
&\text{subject to} \notag \\
    & \forall n : y_n - (x_n^T\omega + b) \leq \epsilon + \xi_n \notag \\
    & \forall n : (x_n^T\omega + b) - y_n\leq \epsilon + \xi_n^* \notag \\
    & \forall n : \xi_n^* \geq 0, \forall n : \xi_n \geq 0 \notag \\ \notag
\end{align} % https://se.mathworks.com/help/stats/understanding-support-vector-machine-regression.html
where the subscript $n$ denotes the $n$-th item in the data set.
\subsubsection{Decision Trees}
\textbf{Bagging Regression} uses the voting scheme of fitting multiple regression decision trees, where each decision tree models a random subset of the original data, and then aggregates there results to achieve higher precision. \\\\
\textbf{Random Forest Regression} ...\\\\
\textbf{Extra Trees Regression} 
\subsubsection{Neural Network}
\textbf{Multilayer perceptron (MLP)} 

\subsection{Preprocessing}
The preprocessing methods used for our results where for example:
\begin{itemize}
    \item Visual inspection of attribute (frequency, mean, std, distribution)
    \item Correlation checks
    \item Dimension reduction methods (PCA, t-SNE, Non-negative transform)
\end{itemize}

\todo{Describe PCA, t-SNE, NNt}


% * Lýsing á aðferðafræði.
% * Stutt lýsing á þeim reikniritum sem eru notuð: flokkarar, aðhvarfsgreiningarlíkön, t-SNE, þyrpingagreining, …
% * Stutt lýsing á gagnasafni/söfnum.
% * Hvernig nákvæmni spálíkana er metin.
% * Hvernig gildi á “hyperparametrum” eru valin
%
% Nákvæmni í lýsingu á að miðast við að samnemendur ykkar í REI602M geti endurtekið tilraunirnar án teljandi vandræða

\section{Results}
\todo{...}

% * Fyrir verkefni #1 og #2: Töflur/myndir sem sýna t.d.
%   * Tíðnirit (e. histograms) fyrir stakar inntaksbreytur/úttaksbreytu, myndir sem sýna fylgni 2ja inntaksbreyta ofl.
%   * Nákvæmni einstakra líkana.
%   * Áhrif “hyperparametra” á gæði líkana.
%   * Yfirlit yfir inntaksbreytur sem mestu máli skipta.

% * Skrifið stutta lýsingu á því sem myndir og töflur sýna. Númerið allar myndir og töflur og notið númer þegar vísað er í úr texta (“Á mynd 2 má sjá …”) Gætið þess texti á myndum sé læsilegur (ekki of lítill).

% * Lýsið stuttlega tilraunum sem skiluðu litlu (“misheppnuðust”)

\section{Conclusion}
\todo{...}

% * Helstu ályktanir.
% * Næstu skref. (Hvernig mynduð þið halda áfram með verkefnið?)

\newpage
\appendix
\section{Appendix}

\subsection{Code}
\label{sec:code}
\lstinputlisting[caption={Python code}, label={lst:code}, language=Python, basicstyle=\small]{../Script.py}

\label{sec:output}
\lstinputlisting[caption={The output from execution of the python code}, label={lst:output}, basicstyle=\small, columns=flexible]{../ScriptOutput.txt}

% For example, you can cite the Nano 3 Lecture notes \cite{nano3}.
% \newpage
% \begin{thebibliography}{9}
% \bibitem{nano3}
%   K. Grove-Rasmussen og Jesper Nygård,
%   \emph{Kvantefænomener i Nanosystemer}.
%   Niels Bohr Institute \& Nano-Science Center, Københavns Universitet
% \end{thebibliography}
\end{document}